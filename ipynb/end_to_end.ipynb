{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference on LDM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules https://github.com/rossiyareich/marching-waifu.git\n",
    "%cd marching-waifu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained DeepDanbooru weights\n",
    "!mkdir ext/weights/\n",
    "%cd ext/weights/\n",
    "!wget https://github.com/KichangKim/DeepDanbooru/releases/download/v3-20211112-sgd-e28/deepdanbooru-v3-20211112-sgd-e28.zip\n",
    "!unzip deepdanbooru-v3-20211112-sgd-e28.zip -d ../AnimeFaceNotebooks/deepdanbooru_model/\n",
    "%cd ../\n",
    "!rm -rf weights\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for local install (run `setup.sh` or `setup.bat` and `huggingface-cli login` beforehand!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!python -m pip install -r requirements.txt\n",
    "!python -m pip install -r ext/Real-ESRGAN/requirements.txt\n",
    "\n",
    "# Create directories\n",
    "!mkdir data/ngp/overview/\n",
    "!mkdir data/ngp/train/\n",
    "\n",
    "# Install local packages\n",
    "%cd ext/Real-ESRGAN/\n",
    "!python setup.py develop\n",
    "%cd ../AnimeFaceNotebooks/DeepDanbooru/\n",
    "!python setup.py develop\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# @title Inference configuration\n",
    "class config(object):\n",
    "    pass\n",
    "\n",
    "# @markdown ###**Models**\n",
    "models = config()\n",
    "models.ldm_repo_id = \"rossiyareich/Nabylon-v1.0-fp16\"  # @param {type: \"string\"}\n",
    "models.ldm_inpaint_repo_id = \"rossiyareich/Nabylon-v1.0-fp16-inpainting\"  # @param {type: \"string\"}\n",
    "models.vae_repo_id = \"stabilityai/sd-vae-ft-mse\"  # @param {type: \"string\"}\n",
    "\n",
    "# @markdown ###**ControlNet**\n",
    "controlnet = config()\n",
    "controlnet.unit_scales = config()\n",
    "controlnet.guidance = config()\n",
    "controlnet.soft_exp = 0.825  # @param {type:\"slider\", min:0, max:1, step:0.025}\n",
    "controlnet.unit_scales.openpose = 1.0  # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "controlnet.unit_scales.depth = 0.8  # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "controlnet.unit_scales.normals = 0.8  # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "controlnet.unit_scales.lineart = 0.6  # @param {type:\"slider\", min:0, max:2, step:0.1}\n",
    "controlnet.guidance.start = 0.0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "controlnet.guidance.end = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "# @markdown ###**DeepDanbooru**\n",
    "deepdanbooru = config()\n",
    "deepdanbooru.threshold = 0.08  # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "deepdanbooru.multiplier = 1.0  # @param {type:\"slider\", min:0, max:4, step:0.1}\n",
    "deepdanbooru.prefix = \"(masterpiece, best quality, {0})+, \"  # @param {type: \"string\"}\n",
    "\n",
    "# @markdown ###**Pipeline**\n",
    "pipeline = config()\n",
    "# @markdown #####**PrereqGen**\n",
    "pipeline.prereq = config()\n",
    "pipeline.prereq.prompt = \"(masterpiece, best quality, dark background, {0})+, 1girl, white hoodie, earmuffs, leggings, white scarf, black gloves, white socks, short blue hair, blue eyes, bangs\"  # @param {type: \"string\"}\n",
    "pipeline.prereq.negative_prompt = \"EasyNegative, (worst quality, low quality, logo, text, watermark, username, nsfw), inaccurate hands and fingers\"  # @param {type: \"string\"}\n",
    "pipeline.prereq.steps = 20  # @param {type:\"slider\", min:1, max:150, step:1}\n",
    "pipeline.prereq.cfg_scale = 10  # @param {type:\"slider\", min:1, max:30, step:0.5}\n",
    "pipeline.prereq.callback_steps = 5  # @param {type:\"slider\", min:0, max:150, step:1}\n",
    "pipeline.prereq.denoising_strength = 1.0  # @param {type:\"slider\", min:0, max:1, step:0.5}\n",
    "pipeline.prereq.seed = -1  # @param {type:\"integer\"}\n",
    "# @markdown #####**RestGen**\n",
    "pipeline.restgen = config()\n",
    "pipeline.restgen.negative_prompt = \"EasyNegative, (worst quality, low quality, logo, text, watermark, username, nsfw), inaccurate hands and fingers\"  # @param {type: \"string\"}\n",
    "pipeline.restgen.steps = 20  # @param {type:\"slider\", min:1, max:150, step:1}\n",
    "pipeline.restgen.cfg_scale = 10  # @param {type:\"slider\", min:1, max:30, step:0.5}\n",
    "pipeline.restgen.callback_steps = 0  # @param {type:\"slider\", min:0, max:150, step:1}\n",
    "pipeline.restgen.denoising_strength = 1.0  # @param {type:\"slider\", min:0, max:1, step:0.5}\n",
    "pipeline.restgen.seed = -1  # @param {type:\"integer\"}\n",
    "pipeline.restgen.inpaint_method = \"original\"  # @param {type:\"string\"}\n",
    "pipeline.restgen.dataset_size = 40  # @param {type:\"integer\"}\n",
    "\n",
    "inference = {\n",
    "    \"models\": {\n",
    "        \"ldm_repo_id\": models.ldm_repo_id,\n",
    "        \"ldm_inpaint_repo_id\": models.ldm_inpaint_repo_id,\n",
    "        \"vae_repo_id\": models.vae_repo_id,\n",
    "    },\n",
    "    \"controlnet\": {\n",
    "        \"soft_exp\": controlnet.soft_exp,\n",
    "        \"unit_scales\": {\n",
    "            \"openpose\": controlnet.unit_scales.openpose,\n",
    "            \"depth\": controlnet.unit_scales.depth,\n",
    "            \"normals\": controlnet.unit_scales.normals,\n",
    "            \"lineart\": controlnet.unit_scales.lineart,\n",
    "        },\n",
    "        \"guidance\": {\n",
    "            \"start\": controlnet.guidance.start,\n",
    "            \"end\": controlnet.guidance.end,\n",
    "        },\n",
    "    },\n",
    "    \"deepdanbooru\": {\n",
    "        \"threshold\": deepdanbooru.threshold,\n",
    "        \"multiplier\": deepdanbooru.multiplier,\n",
    "        \"prefix\": deepdanbooru.prefix,\n",
    "    },\n",
    "    \"pipeline\": {\n",
    "        \"prereq\": {\n",
    "            \"prompt\": pipeline.prereq.prompt,\n",
    "            \"negative_prompt\": pipeline.prereq.negative_prompt,\n",
    "            \"steps\": pipeline.prereq.steps,\n",
    "            \"cfg_scale\": pipeline.prereq.cfg_scale,\n",
    "            \"callback_steps\": pipeline.prereq.callback_steps,\n",
    "            \"denoising_strength\": pipeline.prereq.denoising_strength,\n",
    "            \"seed\": pipeline.prereq.seed,\n",
    "        },\n",
    "        \"restgen\": {\n",
    "            \"negative_prompt\": pipeline.restgen.negative_prompt,\n",
    "            \"steps\": pipeline.restgen.steps,\n",
    "            \"cfg_scale\": pipeline.restgen.cfg_scale,\n",
    "            \"callback_steps\": pipeline.restgen.callback_steps,\n",
    "            \"denoising_strength\": pipeline.restgen.denoising_strength,\n",
    "            \"seed\": pipeline.restgen.seed,\n",
    "            \"inpaint_method\": pipeline.restgen.inpaint_method,\n",
    "            \"dataset_size\": pipeline.restgen.dataset_size,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"inference.json\", \"w\") as f:\n",
    "    json.dump(inference, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from src.utils.file_loader import *\n",
    "from src.utils.image_wrapper import *\n",
    "\n",
    "path = {\n",
    "    \"controlnet_conditions_folder\": \"../data/multi_controlnet/multi_controlnet_data/\",\n",
    "    \"ngp_overview_folder\": \"../data/ngp/overview/\",\n",
    "    \"ngp_train_folder\": \"../data/ngp/train/\",\n",
    "}\n",
    "\n",
    "def display_conditions(conditions, scale):\n",
    "    img = image_wrapper(conditions[0], format=\"pil\").scale(scale)\n",
    "    for img_ in conditions[1:]:\n",
    "        img.concatenate(image_wrapper(img_, format=\"pil\").scale(scale))\n",
    "    display(img.to_pil())\n",
    "\n",
    "def display_at_index(index, filename, scale, show_overview):\n",
    "    img = PIL.Image.open(os.path.join(path[\"ngp_train_folder\"], filename))\n",
    "    display(\n",
    "        image_wrapper(img, format=\"pil\")\n",
    "        .scale(scale)\n",
    "        .to_pil()\n",
    "    )\n",
    "\n",
    "    display_conditions(controlnet_conditions[index], scale)\n",
    "    \n",
    "    if show_overview:\n",
    "        interim = PIL.Image.open(os.path.join(path[\"ngp_overview_folder\"], filename))\n",
    "        display(\n",
    "            image_wrapper(interim, format=\"pil\")\n",
    "            .scale(img.width * scale / interim.width)\n",
    "            .to_pil()\n",
    "        )\n",
    "\n",
    "fl = file_loader()\n",
    "controlnet_conditions = fl.load_controlnet_conditions(path[\"controlnet_conditions_folder\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Prereq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generation_prereq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.25\n",
    "display_at_index(0, \"prereq.png\", scale, pipeline.prereq.callback_steps > 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run RestGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generation_restgen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "scale = 0.25\n",
    "display_interval = 1\n",
    "\n",
    "for i in range(0, pipeline.restgen.dataset_size):\n",
    "    display_at_index(i, f\"{(i+1):04}.png\", scale, pipeline.restgen.callback_steps > 0)\n",
    "    time.sleep(display_interval)\n",
    "    if i < (pipeline.restgen.dataset_size - 1):\n",
    "        clear_output(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training on NeRF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract mesh from trained NeRF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference on T2M-GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retarget animation on to character mesh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marching-waifu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
